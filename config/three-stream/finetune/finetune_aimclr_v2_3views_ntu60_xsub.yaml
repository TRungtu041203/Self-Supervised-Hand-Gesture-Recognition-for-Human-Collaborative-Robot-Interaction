work_dir: ./work_dir/ntu60_cs/finetune

weights: ./work_dir/ntu60_cs/pretext/epoch15_model.pt
ignore_weights: [encoder_q.fc, encoder_q_motion.fc, encoder_q_bone.fc, encoder_k, encoder_k_motion, encoder_k_bone, queue, queue_motion, queue_bone]

# feeder
train_feeder: feeder.ntu_feeder.Feeder_single
train_feeder_args:
  data_path: ./ntu60_frame50/xsub/train_position.npy
  label_path: ./ntu60_frame50/xsub/train_label.pkl
  shear_amplitude: -1
  temperal_padding_ratio: -1
  mmap: True
test_feeder: feeder.ntu_feeder.Feeder_single
test_feeder_args:
  data_path: ./ntu60_frame50/xsub/val_position.npy
  label_path: ./ntu60_frame50/xsub/val_label.pkl
  shear_amplitude: -1
  temperal_padding_ratio: -1
  mmap: True

# model
model: net.aimclr_v2_3views.AimCLR_v2_3views
model_args:
  base_encoder: net.st_gcn.Model
  pretrain: False
  in_channels: 3
  hidden_channels: 16
  hidden_dim: 256
  num_class: 60
  dropout: 0.5
  graph_args:
    layout: 'ntu-rgb+d'
    strategy: 'spatial'
  edge_importance_weighting: True

# optim - Finetune settings
nesterov: True
weight_decay: 0.0001
base_lr: 0.1  # Lower learning rate for finetuning
optimizer: SGD
step: [80]  # Learning rate decreases to 0.01 at epoch 80

# training
device: [0]
batch_size: 128
test_batch_size: 128
num_epoch: 100
stream: 'all'

# log
save_interval: -1
eval_interval: 5

# Finetune: unfreeze all layers
unfreeze_backbone: True 
